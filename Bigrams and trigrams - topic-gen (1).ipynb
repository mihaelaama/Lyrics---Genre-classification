{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c125a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Cleaned_Lyrics  Genre  \\\n",
      "0      Red dragon from the first morning of time\\nRed...  Metal   \n",
      "1      Broad incision sits across the evening\\nA vict...  Metal   \n",
      "2      I dont wanna see\\nI dont wanna say\\nTime stand...  Metal   \n",
      "3      Weve walked together down this winding road\\nI...  Metal   \n",
      "4      Brothers the battle is raging choose your side...  Metal   \n",
      "...                                                  ...    ...   \n",
      "47995  Intro\\nYou did it baby Congratulations homie\\n...    Rap   \n",
      "47996  Tah Murdah\\nPerminently dedicated to the stree...    Rap   \n",
      "47997  Intro\\nCompton in the house\\n\\n\\nChorus DJ Qui...    Rap   \n",
      "47998  Bizzy\\nThere is no way in hell marching factio...    Rap   \n",
      "47999  Produced by Tae Beast\\n\\nIntro\\nMake sure the ...    Rap   \n",
      "\n",
      "       topic_dominant  \n",
      "0                   7  \n",
      "1                   4  \n",
      "2                   4  \n",
      "3                   9  \n",
      "4                   6  \n",
      "...               ...  \n",
      "47995               5  \n",
      "47996               0  \n",
      "47997               1  \n",
      "47998               9  \n",
      "47999               5  \n",
      "\n",
      "[47998 rows x 3 columns]\n",
      "Topic 0: dont know, boom boom, oh oh, know im, just like, yo yo, cause im, dont want, im gonna, nah nah, come come, night day, know love, oh yeah, im just, girl girl, love oh, yeah yeah, know dont, let know\n",
      "Topic 1: la la, hey hey, money money, dont know, cause im, yeah yeah, new york, dont stop, im im, ha ha, dont need, know im, aint got, just like, sing sing, im gonna, think im, day day, like im, feel like\n",
      "Topic 2: yeah yeah, da da, dont know, know know, just like, dont wanna, know im, dont care, fuck fuck, like im, cause im, jump jump, dont want, dont stop, run away, feel like, wanna wanna, im just, make feel, yall niggas\n",
      "Topic 3: oh oh, dont know, yeah yeah, dont care, dont wanna, know im, cause im, baby baby, ive got, im gonna, im just, dont need, know know, oh yeah, youve got, ooh ooh, wont let, like im, dont want, eh eh\n",
      "Topic 4: yeah yeah, dont wanna, im gonna, im just, dont know, know im, want want, right right, oh yeah, im im, wanna know, baby baby, cause im, ive got, dont want, body soul, know youre, like im, feel like, let know\n",
      "Topic 5: dont know, know im, dont want, little bit, aint got, time time, cause im, dont care, im just, better better, turn turn, baby dont, just like, ive got, gonna make, yeah yeah, santa claus, love love, come home, dont stop\n",
      "Topic 6: dont know, dont want, cause im, dont need, dont wanna, know im, like like, just like, feel like, pum pum, know youre, like im, say im, make feel, dont fuck, youre gonna, round round, im sorry, youve got, im gonna\n",
      "Topic 7: love love, na na, im gonna, dont know, yeah yeah, cause im, let let, know im, dont want, like im, dont let, come come, girls girls, feel like, dont care, just want, oh oh, im just, think youre, dont wanna\n",
      "Topic 8: dont know, dont want, know im, im gonna, work work, ooh ooh, im love, dont wanna, way way, like im, just like, know know, hey hey, feel like, cause im, youre gonna, wont let, think im, just wanna, im just\n",
      "Topic 9: ive got, dont know, know im, just like, dont let, im just, gimme gimme, ah ah, uh uh, im gonna, say say, gotta gotta, know know, youve got, feel like, im like, love baby, ive seen, love love, fly away\n",
      "topic_dominant     0     1     2     3     4     5     6     7     8     9\n",
      "Genre                                                                     \n",
      "Jazz            1221  1169   973  1200  1331  1325  1157  1343  1106  1175\n",
      "Metal           1156  1137  1050  1130  1260  1167  1312  1134  1354  1300\n",
      "Pop             1078  1030  1095  1455  1393  1107  1125  1264  1369  1084\n",
      "Rap             1189  1222  1320  1261  1181  1105  1175  1224  1245  1076\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('12000_lyrics_dataset.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'Cleaned_Lyrics' column\n",
    "data = data.dropna(subset=['Cleaned_Lyrics'])\n",
    "\n",
    "# Initialize the vectorizer with bigrams and fit it to the text data\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(data['Cleaned_Lyrics'])\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "\n",
    "# Fit the LDA model to the vectorized data\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the dominant topic for each instance\n",
    "topic_distributions = lda.transform(X)\n",
    "dominant_topics = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# Add the dominant topic to the original dataset\n",
    "data['topic_dominant'] = dominant_topics\n",
    "\n",
    "# Print the initial results\n",
    "print(data[['Cleaned_Lyrics', 'Genre', 'topic_dominant']])\n",
    "\n",
    "# Function to get the top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Get the top bigrams for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_bigrams = get_top_words(lda, feature_names, 20)\n",
    "\n",
    "# Print the top bigrams for each topic\n",
    "for topic, bigrams in top_bigrams.items():\n",
    "    print(f\"Topic {topic}: {', '.join(bigrams)}\")\n",
    "\n",
    "# Save the top bigrams for each topic to a CSV file\n",
    "with open('top_bigrams.csv', 'w') as f:\n",
    "    f.write(\"Topic,Bigrams\\n\")\n",
    "    for topic, bigrams in top_bigrams.items():\n",
    "        f.write(f\"{topic},{', '.join(bigrams)}\\n\")\n",
    "\n",
    "# Analyze the frequency of topics per genre\n",
    "topic_genre_distribution = data.groupby(['Genre', 'topic_dominant']).size().unstack(fill_value=0)\n",
    "\n",
    "# Print the distribution of topics per genre\n",
    "print(topic_genre_distribution)\n",
    "\n",
    "# Save the distribution of topics per genre to a CSV file\n",
    "topic_genre_distribution.to_csv('bigrams_12000_topic_genre_distribution.csv')\n",
    "\n",
    "# Save the original dataset with dominant topics to a CSV file\n",
    "data.to_csv('bigrams_12000_lyrics_with_topics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29963c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Cleaned_Lyrics  Genre  \\\n",
      "0      Red dragon from the first morning of time\\nRed...  Metal   \n",
      "1      Broad incision sits across the evening\\nA vict...  Metal   \n",
      "2      I dont wanna see\\nI dont wanna say\\nTime stand...  Metal   \n",
      "3      Weve walked together down this winding road\\nI...  Metal   \n",
      "4      Brothers the battle is raging choose your side...  Metal   \n",
      "...                                                  ...    ...   \n",
      "47995  Intro\\nYou did it baby Congratulations homie\\n...    Rap   \n",
      "47996  Tah Murdah\\nPerminently dedicated to the stree...    Rap   \n",
      "47997  Intro\\nCompton in the house\\n\\n\\nChorus DJ Qui...    Rap   \n",
      "47998  Bizzy\\nThere is no way in hell marching factio...    Rap   \n",
      "47999  Produced by Tae Beast\\n\\nIntro\\nMake sure the ...    Rap   \n",
      "\n",
      "       topic_dominant  \n",
      "0                   6  \n",
      "1                   3  \n",
      "2                   2  \n",
      "3                   3  \n",
      "4                   6  \n",
      "...               ...  \n",
      "47995               3  \n",
      "47996               9  \n",
      "47997               3  \n",
      "47998               7  \n",
      "47999               8  \n",
      "\n",
      "[47998 rows x 3 columns]\n",
      "Topic 0: oh oh oh, na na na, come come come, yeah yeah yeah, girls girls girls, love love love, right right right, sing sing sing, low speak love, speak low speak, let let let, oh yeah yeah, love oh love, oh love oh, yeah yeah oh, cmon cmon cmon, nigga nigga nigga, uh uh uh, soon speak low, oh oh yeah\n",
      "Topic 1: yeah yeah yeah, boom boom boom, baby baby baby, say say say, beat beat beat, merry little christmas, know know know, love im love, love love love, wanna wanna wanna, dreams come true, im love im, hey hey hey, fight fight fight, alright alright alright, oh yeah oh, dont wanna know, dont know im, sweet lovely sweeter, lovely sweeter roses\n",
      "Topic 2: da da da, better better better, turn turn turn, yeah yeah yeah, ay oh ay, girl girl girl, oh ay oh, just just just, yeah oh yeah, ya ya ya, got got got, im coming home, oh yeah oh, day day day, yah yah yah, low low low, know know know, ay ay ay, dat dat dat, ooh ooh ooh\n",
      "Topic 3: la la la, love love love, yeah yeah yeah, santa claus comin, claus comin town, heart singing lover, green dolphin street, uh uh uh, shake shake shake, dont know im, got got got, moon new love, singing lover come, hello hello hello, black yellow black, yellow black yellow, movin movin movin, left right left, high high high, better watch better\n",
      "Topic 4: want want want, fuck fuck fuck, rollin rollin rollin, come let adore, bye bye blackbird, deep blue sea, bye bye bye, ha ha ha, hot hot hot, new york new, york new york, love understand oh, hard luck stories, woah woah woah, luck stories hand, oh hard luck, understand oh hard, devil deep blue, yes yes yes, yeah yeah yeah\n",
      "Topic 5: hey hey hey, know dont know, know know know, dont know dont, foolish things remind, yo yo yo, whoa whoa whoa, im im im, like like like, time time time, lady good oh, oh oh oh, blue jeans youre, youre anybodys baby, baby mamas gone, sophisticated lady blue, anybodys baby mamas, lady blue jeans, jeans youre anybodys, dumb dumb dumb\n",
      "Topic 6: yeah yeah yeah, work work work, nah nah nah, round round round, hallelujah hallelujah hallelujah, wake wake wake, ohh ohh ohh, catch ridin dirty, im body soul, surrender body soul, youre making know, making know im, gladly surrender body, wreck youre making, life wreck youre, id gladly surrender, taking id gladly, ha ha ha, old black magic, tryna catch ridin\n",
      "Topic 7: ah ah ah, run run run, check check check, yall yall yall, boom boom boom, just little bit, ding dong ding, doo doo doo, ho ho ho, hey hey hey, fall love easily, easily fall love, love easily fall, fall love fast, gone gone gone, long long long, squeeze dont tease, dont know got, women lie men, lie men lie\n",
      "Topic 8: eh eh eh, yeah yeah yeah, gimme gimme gimme, jump jump jump, dont know love, bang bang bang, pum pum pum, hey hey hey, dont dont dont, mm mm mm, love love love, got love got, let good times, heart heart heart, good times roll, dont know im, nothin till hear, white white white, moonglow way blue, tiger wheres tiger\n",
      "Topic 9: love love love, ooh ooh ooh, ha ha ha, let let let, gotta gotta gotta, breathe breathe breathe, fly rainbow oh, dont stop dont, stop dont stop, hey hey hey, im gonna love, rainy day funny, believe youre love, ocean high sky, deep ocean high, yo yo yo, em em em, uh huh uh, yeah yeah yeah, dont wanna die\n",
      "topic_dominant     0     1     2     3     4     5     6     7     8     9\n",
      "Genre                                                                     \n",
      "Jazz            1466  1118  1097   995  1011  1356  1353  1145  1164  1295\n",
      "Metal           2394  1078  1075  1131  1031  1050  1057  1065  1064  1055\n",
      "Pop             1600  1149  1118  1227  1110  1176  1073  1083  1238  1226\n",
      "Rap             1248  1153  1184  1246  1093  1238  1172  1198  1205  1261\n"
     ]
    }
   ],
   "source": [
    "                        import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('12000_lyrics_dataset.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'Cleaned_Lyrics' column\n",
    "data = data.dropna(subset=['Cleaned_Lyrics'])\n",
    "\n",
    "# Initialize the vectorizer with trigrams and fit it to the text data\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(data['Cleaned_Lyrics'])\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "\n",
    "# Fit the LDA model to the vectorized data\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the dominant topic for each instance\n",
    "topic_distributions = lda.transform(X)\n",
    "dominant_topics = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# Add the dominant topic to the original dataset\n",
    "data['topic_dominant'] = dominant_topics\n",
    "\n",
    "# Print the initial results\n",
    "print(data[['Cleaned_Lyrics', 'Genre', 'topic_dominant']])\n",
    "\n",
    "# Function to get the top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Get the top trigrams for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_trigrams = get_top_words(lda, feature_names, 20)\n",
    "\n",
    "# Print the top trigrams for each topic\n",
    "for topic, trigrams in top_trigrams.items():\n",
    "    print(f\"Topic {topic}: {', '.join(trigrams)}\")\n",
    "\n",
    "# Save the top trigrams for each topic to a CSV file\n",
    "with open('top_trigrams.csv', 'w') as f:\n",
    "    f.write(\"Topic,Trigrams\\n\")\n",
    "    for topic, trigrams in top_trigrams.items():\n",
    "        f.write(f\"{topic},{', '.join(trigrams)}\\n\")\n",
    "\n",
    "# Analyze the frequency of topics per genre\n",
    "topic_genre_distribution = data.groupby(['Genre', 'topic_dominant']).size().unstack(fill_value=0)\n",
    "\n",
    "# Print the distribution of topics per genre\n",
    "print(topic_genre_distribution)\n",
    "\n",
    "# Save the distribution of topics per genre to a CSV file\n",
    "topic_genre_distribution.to_csv('trigrams_12000_topic_genre_distribution.csv')\n",
    "\n",
    "# Save the original dataset with dominant topics to a CSV file\n",
    "data.to_csv('trigrams_12000_lyrics_with_topics.csv', index=False)\n",
    "                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1fe6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
