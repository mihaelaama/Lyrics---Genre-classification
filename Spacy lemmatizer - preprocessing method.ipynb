{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e159802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: love, heart, know, night, say, like, baby, make, feel, oh, come, kiss, look, blue, ill, dream, day, tell, time, song\n",
      "Topic 1: world, fly, burn, pretty, young, star, sky, hand, night, life, time, tonight, rise, reach, revolution, alright, chorus, come, drunk, like\n",
      "Topic 2: na, like, wan, em, come, know, cause, verse, man, ya, make, little, let, think, hook, bone, hard, beat, say, die\n",
      "Topic 3: know, oh, say, yeah, want, baby, time, let, come, way, cause, like, tell, make, feel, girl, need, right, ill, think\n",
      "Topic 4: time, die, away, come, fall, end, lose, world, life, break, leave, blood, hell, live, stand, light, inside, war, fight, feel\n",
      "Topic 5: day, life, night, come, way, ill, know, turn, oh, rain, time, sleep, wait, live, away, make, like, pain, man, think\n",
      "Topic 6: bitch, like, ya, money, ass, yeah, know, verse, man, come, roll, say, da, dick, cause, em, pussy, yo, ai, chorus\n",
      "Topic 7: like, say, tell, cause, verse, yeah, know, live, time, mind, people, em, make, high, think, come, life, let, look, good\n",
      "Topic 8: la, dance, pour, en, white, hallelujah, hey, son, te, tu, floor, mon, sur, si, se, night, little, ne, ya, ton\n",
      "Topic 9: like, know, make, cause, yo, man, right, verse, yeah, big, come, ya, hook, em, chorus, bout, say, stop, new, rock\n",
      "topic_dominant    0   1    2    3    4    5    6    7   8    9\n",
      "Genre                                                         \n",
      "Jazz            515  73   49  238   82  121   22   47  18   35\n",
      "Metal            40  58   33  332  450  154   12   64  14   43\n",
      "Pop             170  39   73  531   68   82   58   55  20  104\n",
      "Rap               8   9  173  108   11   27  126  193  31  514\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Asigură-te că stopwords și corpus-ul de cuvinte sunt descărcate\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "# Încarcă modelul de limbă engleză din spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Încarcă setul de date\n",
    "data = pd.read_csv('balanced_lyrics_dataset.csv')\n",
    "\n",
    "# Setează stopwords în engleză și lista de cuvinte în engleză\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(words.words())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Păstrează doar caracterele alfabetice și convertește la lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    # Tokenizează textul\n",
    "    words = word_tokenize(text)\n",
    "    # Elimină stopwords și păstrează doar cuvintele în engleză\n",
    "    words = [word for word in words if word not in stop_words and word in english_words]\n",
    "    # Concatenează cuvintele pentru lemmatizare\n",
    "    text = ' '.join(words)\n",
    "    # Aplica lemmatizare folosind spacy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc if token.lemma_ not in stop_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Aplică funcția de preprocesare pe coloana 'Cleaned_Lyrics'\n",
    "data['Cleaned_Lyrics'] = data['Cleaned_Lyrics'].apply(preprocess_text)\n",
    "\n",
    "# Salvează setul de date preprocesat\n",
    "data.to_csv('preprocessed_lyrics_dataset.csv', index=False)\n",
    "\n",
    "# Initializează vectorizatorul și modelul LDA\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "\n",
    "# Vectorizează textele din coloana 'Cleaned_Lyrics'\n",
    "X = vectorizer.fit_transform(data['Cleaned_Lyrics'])\n",
    "\n",
    "# Antrenează modelul LDA pe datele vectorizate\n",
    "lda.fit(X)\n",
    "\n",
    "# Obține distribuția topic-urilor pentru fiecare instanță de versuri\n",
    "topic_distributions = lda.transform(X)\n",
    "\n",
    "# Identifică topic-ul dominant pentru fiecare instanță\n",
    "dominant_topics = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# Adaugă topic-ul dominant la setul de date original\n",
    "data['topic_dominant'] = dominant_topics\n",
    "\n",
    "# Funcție pentru a obține cuvintele cele mai relevante pentru fiecare topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Obține cuvintele corespunzătoare fiecărui topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = get_top_words(lda, feature_names, 20)  # 20 de cuvinte pentru fiecare topic\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_top_words = pd.DataFrame(dict([(f'Topic {i}', top_words[i]) for i in top_words]))\n",
    "\n",
    "# Salvează cuvintele predominante într-un fișier CSV\n",
    "df_top_words.to_csv('nou_top_words_per_topic.csv', index=False)\n",
    "\n",
    "# Afișează cuvintele pentru fiecare topic\n",
    "for topic, words in top_words.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")\n",
    "\n",
    "# Analiza frecvenței topicurilor pe genuri\n",
    "topic_genre_distribution = data.groupby(['Genre', 'topic_dominant']).size().unstack(fill_value=0)\n",
    "\n",
    "# Afișează distribuția topicurilor pe genuri\n",
    "print(topic_genre_distribution)\n",
    "\n",
    "# Salvează distribuția topicurilor pe genuri într-un fișier CSV\n",
    "topic_genre_distribution.to_csv('nou_topic_genre_distribution.csv')\n",
    "\n",
    "# Salvează datele modificate într-un fișier CSV\n",
    "data.to_csv('nou_dataset_with_dominant_topics.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44287ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
