{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2692d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test dataset saved successfully as CSV.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "input_path = 'test.csv'\n",
    "output_path = 'clean_test_dataset.csv'\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['Artist', 'Song', 'Genre']\n",
    "\n",
    "# Open the input file and read it\n",
    "with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "    data_reader = csv.DictReader(input_file)\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        fieldnames = columns_to_keep + ['Cleaned_Lyrics']  # Add a new field for the cleaned lyrics\n",
    "        data_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        data_writer.writeheader()  # Write the header\n",
    "        \n",
    "        # Iterate over each row in the input data\n",
    "        for row in data_reader:\n",
    "            # Filter out the unwanted columns and the 'Lyrics' column\n",
    "            filtered_row = {key: row[key] for key in columns_to_keep}\n",
    "            \n",
    "            # Check if the 'Lyrics' column is empty\n",
    "            if not row['Lyrics']:\n",
    "                continue\n",
    "            \n",
    "            # Clean and preprocess the lyrics\n",
    "            # Tokenization\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokenized_lyrics = tokenizer.tokenize(row['Lyrics'].lower())\n",
    "            \n",
    "            # Filtering out short tokens and non-alphanumeric tokens\n",
    "            filtered_tokens = [token for token in tokenized_lyrics if len(token) > 2 and not token.isnumeric()]\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            \n",
    "            # Stop words removal\n",
    "            stop_words = stopwords.words('english')\n",
    "            new_stop_words = ['ooh', 'yeah', 'hey', 'whoa', 'woah', 'ohh', 'was', 'mmm', 'oooh', 'yah', 'yeh', 'mmm', 'hmm', 'deh', 'doh', 'jah', 'wa']\n",
    "            stop_words.extend(new_stop_words)\n",
    "            cleaned_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "            \n",
    "            # Filtering out non-meaningful words\n",
    "            non_meaningful_words = ['aaaa', 'aaah', 'aaaah', 'oooooh', 'ooooooooh', 'oooooooooh', 'aaalways', 'oooooooooh', 'aah', 'aachoo', 'accident', 'alalalalchemist', 'aback', 'aboard', 'ghost', 'abacus', 'absurd', 'across', 'log', 'abominal', 'aaaaaaaalllllright', 'aaaaaaaaaaaaaa', 'aaaaaaaaaaaaaah', 'aaaaaaaah', 'baldheaded', 'aaaaaaaaaaaaaaa', 'aaaaaaaaggghhhhhhhhhhhhhh', 'aaaaaaaaah', 'afrikaa', 'aan', 'aback', 'abnoxious']\n",
    "            cleaned_lyrics = ' '.join(token for token in cleaned_tokens if token not in non_meaningful_words)\n",
    "            \n",
    "            # Update the row with the cleaned lyrics\n",
    "            filtered_row['Cleaned_Lyrics'] = cleaned_lyrics\n",
    "            \n",
    "            # Write the row to the output file\n",
    "            data_writer.writerow(filtered_row)\n",
    "\n",
    "print(\"Clean test dataset saved successfully as CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700ae70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each genre in the cleaned dataset:\n",
      "Hip-Hop: 960\n",
      "Indie: 510\n",
      "Metal: 810\n",
      "Pop: 1110\n",
      "Country: 810\n",
      "Jazz: 660\n",
      "Rock: 1410\n",
      "R&B: 510\n",
      "Electronic: 660\n",
      "Folk: 495\n"
     ]
    }
   ],
   "source": [
    "  from collections import Counter\n",
    "\n",
    "# Open the cleaned CSV file and read it\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    genre_counts = Counter(row['Genre'] for row in reader)\n",
    "\n",
    "# Print the count of each genre\n",
    "print(\"Count of each genre in the cleaned dataset:\")\n",
    "for genre, count in genre_counts.items():\n",
    "    print(f\"{genre}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf0be190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test dataset saved successfully as CSV.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "input_path = '12000_lyrics_dataset.csv'\n",
    "output_path = 'clean_train_dataset.csv'\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['Artist', 'Title', 'Genre']\n",
    "\n",
    "# Open the input file and read it\n",
    "with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "    data_reader = csv.DictReader(input_file)\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        fieldnames = columns_to_keep + ['Cleaned_Lyrics']  # Add a new field for the cleaned lyrics\n",
    "        data_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        data_writer.writeheader()  # Write the header\n",
    "        \n",
    "        # Iterate over each row in the input data\n",
    "        for row in data_reader:\n",
    "            # Filter out the unwanted columns and the 'Lyrics' column\n",
    "            filtered_row = {key: row[key] for key in columns_to_keep}\n",
    "            \n",
    "            # Check if the 'Lyrics' column is empty\n",
    "            if not row['Cleaned_Lyrics']:\n",
    "                continue\n",
    "            \n",
    "            # Clean and preprocess the lyrics\n",
    "            # Tokenization\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokenized_lyrics = tokenizer.tokenize(row['Cleaned_Lyrics'].lower())\n",
    "            \n",
    "            # Filtering out short tokens and non-alphanumeric tokens\n",
    "            filtered_tokens = [token for token in tokenized_lyrics if len(token) > 2 and not token.isnumeric()]\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            \n",
    "            # Stop words removal\n",
    "            stop_words = stopwords.words('english')\n",
    "            new_stop_words = ['ooh', 'yeah', 'hey', 'whoa', 'woah', 'ohh', 'was', 'mmm', 'oooh', 'yah', 'yeh', 'mmm', 'hmm', 'deh', 'doh', 'jah', 'wa']\n",
    "            stop_words.extend(new_stop_words)\n",
    "            cleaned_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "            \n",
    "            # Filtering out non-meaningful words\n",
    "            non_meaningful_words = ['aaaa', 'aaah', 'aaaah', 'oooooh', 'ooooooooh', 'oooooooooh', 'aaalways', 'oooooooooh', 'aah', 'aachoo', 'accident', 'alalalalchemist', 'aback', 'aboard', 'ghost', 'abacus', 'absurd', 'across', 'log', 'abominal', 'aaaaaaaalllllright', 'aaaaaaaaaaaaaa', 'aaaaaaaaaaaaaah', 'aaaaaaaah', 'baldheaded', 'aaaaaaaaaaaaaaa', 'aaaaaaaaggghhhhhhhhhhhhhh', 'aaaaaaaaah', 'afrikaa', 'aan', 'aback', 'abnoxious']\n",
    "            cleaned_lyrics = ' '.join(token for token in cleaned_tokens if token not in non_meaningful_words)\n",
    "            \n",
    "            # Update the row with the cleaned lyrics\n",
    "            filtered_row['Cleaned_Lyrics'] = cleaned_lyrics\n",
    "            \n",
    "            # Write the row to the output file\n",
    "            data_writer.writerow(filtered_row)\n",
    "\n",
    "print(\"Clean train dataset saved successfully as CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cec31aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered test dataset saved successfully as filtered_test_dataset.csv.\n",
      "Balanced test dataset saved successfully as balanced_test_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "input_path = 'clean_test_dataset.csv'\n",
    "filtered_output_path = 'filtered_test_dataset.csv'\n",
    "balanced_output_path = 'balanced_test_dataset.csv'\n",
    "num_samples_per_genre = 660\n",
    "\n",
    "# Genurile dorite\n",
    "desired_genres = {'Metal', 'Rap', 'Pop', 'Jazz'}\n",
    "\n",
    "# Citește setul de date curățat și filtrează doar genurile dorite, înlocuind \"Hip-Hop\" cu \"Rap\"\n",
    "filtered_data = []\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        genre = row['Genre']\n",
    "        if genre == 'Hip-Hop':\n",
    "            genre = 'Rap'\n",
    "        if genre in desired_genres:\n",
    "            row['Genre'] = genre\n",
    "            filtered_data.append(row)\n",
    "\n",
    "# Scrie setul de date filtrat într-un fișier CSV\n",
    "fieldnames = ['Artist', 'Title', 'Genre', 'Cleaned_Lyrics']\n",
    "with open(filtered_output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "print(f\"Filtered test dataset saved successfully as {filtered_output_path}.\")\n",
    "\n",
    "# Echilibrează setul de date\n",
    "data_by_genre = defaultdict(list)\n",
    "for row in filtered_data:\n",
    "    data_by_genre[row['Genre']].append(row)\n",
    "\n",
    "balanced_data = []\n",
    "for genre, rows in data_by_genre.items():\n",
    "    if len(rows) >= num_samples_per_genre:\n",
    "        balanced_data.extend(random.sample(rows, num_samples_per_genre))\n",
    "    else:\n",
    "        balanced_data.extend(rows)\n",
    "\n",
    "# Scrie setul de date echilibrat într-un fișier CSV\n",
    "with open(balanced_output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(balanced_data)\n",
    "\n",
    "print(f\"Balanced test dataset saved successfully as {balanced_output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94535755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered test dataset saved successfully as filtered_test_dataset2.csv.\n",
      "Balanced test dataset saved successfully as balanced_test_dataset2.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "input_path = 'clean_test_dataset.csv'\n",
    "filtered_output_path = 'filtered_test_dataset2.csv'\n",
    "balanced_output_path = 'balanced_test_dataset2.csv'\n",
    "num_samples_per_genre = 510\n",
    "\n",
    "# Genurile dorite\n",
    "desired_genres = {'Metal', 'Rap', 'Pop', 'Jazz', 'Indie', 'Country', 'R&B', 'Electronic'}\n",
    "\n",
    "# Citește setul de date curățat și filtrează doar genurile dorite, înlocuind \"Hip-Hop\" cu \"Rap\"\n",
    "filtered_data = []\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        genre = row['Genre']\n",
    "        if genre == 'Hip-Hop':\n",
    "            genre = 'Rap'\n",
    "        if genre in desired_genres:\n",
    "            row['Genre'] = genre\n",
    "            filtered_data.append(row)\n",
    "            \n",
    "# Scrie setul de date filtrat într-un fișier CSV\n",
    "fieldnames = ['Artist', 'Title', 'Genre', 'Cleaned_Lyrics']\n",
    "with open(filtered_output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "print(f\"Filtered test dataset saved successfully as {filtered_output_path}.\")\n",
    "\n",
    "# Echilibrează setul de date\n",
    "data_by_genre = defaultdict(list)\n",
    "for row in filtered_data:\n",
    "    data_by_genre[row['Genre']].append(row)\n",
    "\n",
    "balanced_data = []\n",
    "for genre, rows in data_by_genre.items():\n",
    "    if len(rows) >= num_samples_per_genre:\n",
    "        balanced_data.extend(random.sample(rows, num_samples_per_genre))\n",
    "    else:\n",
    "        balanced_data.extend(rows)\n",
    "\n",
    "# Scrie setul de date echilibrat într-un fișier CSV\n",
    "with open(balanced_output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(balanced_data)\n",
    "\n",
    "print(f\"Balanced test dataset saved successfully as {balanced_output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca77d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
